{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumpy version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, numpy\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pandas\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\compat\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     is_numpy_dev,\n\u001b[0;32m     20\u001b[0m     np_version_under1p21,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     pa_version_under1p01,\n\u001b[0;32m     24\u001b[0m     pa_version_under2p0,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     pa_version_under9p0,\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\compat\\numpy\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[0;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     Appender,\n\u001b[0;32m      4\u001b[0m     Substitution,\n\u001b[0;32m      5\u001b[0m     cache_readonly,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     hash_array,\n\u001b[0;32m     10\u001b[0m     hash_pandas_object,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:14\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     Any,\n\u001b[0;32m      8\u001b[0m     Callable,\n\u001b[0;32m      9\u001b[0m     Mapping,\n\u001b[0;32m     10\u001b[0m     cast,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     F,\n\u001b[0;32m     17\u001b[0m     T,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\__init__.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     NaT,\n\u001b[0;32m     16\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     iNaT,\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task1: Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OybR0FpqTKgX"
   },
   "outputs": [],
   "source": [
    "google_colab = False\n",
    "\n",
    "local_root = 'Project2-data'\n",
    "colab_root = '/content/drive/MyDrive/Colab Notebooks/Project2-data/'\n",
    "root = colab_root if google_colab else local_root\n",
    "\n",
    "users = pd.read_csv(f'{root}/users.txt', sep=' ', names=['user_id'], header=None)\n",
    "movies = pd.read_csv(f'{root}/movie_titles.txt', names=['movie_id', 'year', 'title'], header=None, on_bad_lines='skip')\n",
    "movie_ids = movies['movie_id']\n",
    "\n",
    "ratings_train = pd.read_csv(f'{root}/netflix_train.txt', sep=' ', names=['user_id', 'movie_id', 'rating', 'date'], header=None)\n",
    "ratings_test = pd.read_csv(f'{root}/netflix_test.txt', sep=' ', names=['user_id', 'movie_id', 'rating', 'date'], header=None)\n",
    "\n",
    "# We create an empty matrix of the size [users, movie_ids]\n",
    "# Here empty values are set to 0 and assume there is maximum one review per movie per user\n",
    "matrix = ratings_train.pivot(index='user_id', columns='movie_id', values='rating').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task2: Collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgVBfh13TaCO"
   },
   "outputs": [],
   "source": [
    "def userCF(matrix, user_id, movie_id, filter_only_rated=True):\n",
    "    user_i_ratings = matrix.loc[user_id]\n",
    "    k = 0\n",
    "    top = 0\n",
    "    bottom = 0\n",
    "\n",
    "    # To hinder users being treated as a Series, we convert them to a DataFrame\n",
    "    user_df = users['user_id']\n",
    "    user_df_filtered = user_df[user_df != user_id]\n",
    "\n",
    "    # We estimate the similarity sum between the user and all other users\n",
    "    for user_k_id in user_df_filtered:\n",
    "        user_k_ratings = matrix.loc[user_k_id]\n",
    "\n",
    "        # If filter_only_rated, we don't account for people who haven't rated the movie\n",
    "        if filter_only_rated and (user_k_ratings[movie_id] == 0):\n",
    "            continue\n",
    "\n",
    "        # Calculate the cosine similarity between the ratings\n",
    "        similarity = np.dot(user_i_ratings, user_k_ratings) / (np.linalg.norm(user_i_ratings) * np.linalg.norm(user_k_ratings))\n",
    "\n",
    "        top += (similarity * user_k_ratings[movie_id])\n",
    "        bottom += similarity\n",
    "\n",
    "        # We increment k to see how many people\n",
    "        k+=1\n",
    "\n",
    "    return top/bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNjzhz7bTXsM"
   },
   "outputs": [],
   "source": [
    "# Check the difference from the testset and our estimate\n",
    "def checkError(estimated, actual):\n",
    "    return np.absolute(estimated-actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkMnIA-eTUBF"
   },
   "outputs": [],
   "source": [
    "# ----- Testing the userCF function -----\n",
    "def userCF_rmse(test_size=10):\n",
    "    # Sample a subset of the ratings_test for faster computation\n",
    "    sampled_test = ratings_test.sample(n=test_size, random_state=42)\n",
    "\n",
    "    # Calculate deviations in a vectorized form\n",
    "    deviations = []\n",
    "    for _, data_real in sampled_test.iterrows():\n",
    "        data_estimated = userCF(matrix, data_real['user_id'], data_real['movie_id'])\n",
    "        deviation = (data_real['rating'] - data_estimated) ** 2\n",
    "        deviations.append(deviation)\n",
    "\n",
    "    return np.sqrt(np.mean(deviations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cX7-_c8qTRh6",
    "outputId": "3da4a7ff-075a-48a0-b1b2-6530f7e5c0c0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(\"----- RMSE -----\")\n",
    "for i in range(1000, 100000, 10000):\n",
    "    print(f\" size %4s: %10.2f\" % (i, userCF_rmse(i)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3: Matrix Decomposition Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_decomposition(X_train, X_test, k=50, lambda_=0.5, alpha=1e-3, max_iter=100, tolerance=1e-4):\n",
    "    # Initialize U and V matrices with small random values\n",
    "    num_users, num_movies = X_train.shape\n",
    "    U = np.random.rand(num_users, k)\n",
    "    V = np.random.rand(num_movies, k)\n",
    "\n",
    "    # Create an indicator matrix A (1 for known values, 0 for unknown values)\n",
    "    A = (X_train > 0).astype(int)\n",
    "\n",
    "    # To store the previous value of the objective function for convergence check\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        # Compute the predicted ratings matrix (U * V.T)\n",
    "        X_pred = np.dot(U, V.T)\n",
    "\n",
    "        # Calculate the loss function J (with Frobenius norm and regularization)\n",
    "        error = A * (X_train - X_pred)\n",
    "        loss = 0.5 * np.sum(error**2) + lambda_ * (np.sum(U**2) + np.sum(V**2))\n",
    "\n",
    "        # Compute gradients with respect to U and V\n",
    "        grad_U = -np.dot(error, V) + 2 * lambda_ * U\n",
    "        grad_V = -np.dot(error.T, U) + 2 * lambda_ * V\n",
    "\n",
    "        # Update U and V using gradient descent\n",
    "        U -= alpha * grad_U\n",
    "        V -= alpha * grad_V\n",
    "\n",
    "        # Check for convergence by comparing the change in loss function\n",
    "        if np.abs(prev_loss - loss) < tolerance:\n",
    "            print(f\"Convergence reached at iteration {iteration + 1}\")\n",
    "            break\n",
    "\n",
    "        prev_loss = loss\n",
    "\n",
    "        # Optionally: Print the loss at every iteration to monitor the progress\n",
    "        if (iteration==0 or iteration==max_iter-1):\n",
    "            print(f\"     {iteration}: {loss:.4f}\")\n",
    "\n",
    "    # Compute RMSE on the test set\n",
    "    rmse = calculate_RMSE(X_pred, X_test)\n",
    "\n",
    "    return U, V, X_pred, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_RMSE(X_pred, X_test):\n",
    "    mask = X_test > 0\n",
    "    error = (X_pred - X_test) ** 2\n",
    "    rmse = np.sqrt(np.sum(error[mask]) / np.sum(mask))\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_predictions(X_pred, X_test, n=5):\n",
    "    # Get indices of non-zero ratings in X_test\n",
    "    non_zero_indices = np.where(X_test > 0)\n",
    "\n",
    "    # Ensure we have enough non-zero ratings\n",
    "    if len(non_zero_indices[0]) < n:\n",
    "        print(\"Not enough non-zero ratings to sample.\")\n",
    "        return\n",
    "\n",
    "    # Sample n random indices from the non-zero ratings\n",
    "    random_indices = random.sample(range(len(non_zero_indices[0])), n)\n",
    "\n",
    "    print(f\" --- Testing 5 random Movie-User Pair ---\")\n",
    "    for idx in random_indices:\n",
    "        user = non_zero_indices[0][idx]\n",
    "        movie = non_zero_indices[1][idx]\n",
    "\n",
    "        # We display the predicted and actual value for this pair\n",
    "        print(f\"Movie: {movie_ids[movie]}, User: {users['user_id'][user]}: Predicted: {X_pred[user, movie]:.2f}, Actual: {X_test[user, movie]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create X_test from the test data (same as X_train structure)\n",
    "X_train = matrix.values  # Use the pivot table from earlier\n",
    "X_test = ratings_test.pivot(index='user_id', columns='movie_id', values='rating').fillna(0)\n",
    "\n",
    "# The testdata actually miss some values\n",
    "# We take every movie and user found in the training set, not present in the testset, and add these with the values 0\n",
    "X_test = X_test.reindex(columns=matrix.columns, fill_value=0)\n",
    "\n",
    "# Setup\n",
    "best_rmse = np.inf\n",
    "X_test_np = X_test.values   # To speed up calculations I've used numpy arrays instead of pandas dataframes\n",
    "\n",
    "# Testing the algorithm with different k and alpha\n",
    "print(\"Started algorithm\")\n",
    "\n",
    "for a in [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]:\n",
    "    for k in range(20, 101, 20):\n",
    "        for lambda_ in [1e-3, 1e-2, 1e-1, 5e-1, 1]:\n",
    "          for max_iter in [50, 100, 200]:\n",
    "            print(f\"     k={k}, alpha={a}, lambda={lambda_}\")\n",
    "            U, V, X_pred, test_rmse = matrix_decomposition(X_train, X_test_np, k, lambda_, a, max_iter=max_iter)\n",
    "\n",
    "            # Track the best combination based on RMSE\n",
    "            if test_rmse < best_rmse:\n",
    "                best_rmse = test_rmse\n",
    "                best_k = k\n",
    "                best_lambda = lambda_\n",
    "                best_alpha = a\n",
    "                print(f\"New best parameters found: k={best_k}, alpha={best_alpha}, lambda={best_lambda}, RMSE={best_rmse:.4f}, max_iter={max_iter}\")\n",
    "                if(best_rmse<1):\n",
    "                  check_predictions(X_pred, X_test_np, n=5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
