{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XZvMRS6ohcZs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.metrics import SparseCategoricalAccuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7GY2Z24H3eq"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "U054Vbi7H9OY"
      },
      "outputs": [],
      "source": [
        "def LoadData(num_classes = 50, num_samples_per_class_train = 15, num_samples_per_class_test = 5, seed = 1, flatten=False, is_colab=False):\n",
        "    \"\"\"\n",
        "    Load data and split it into training and testing\n",
        "    Args:\n",
        "        num_classes: number of classes adopted, -1 represents using all the classes\n",
        "        num_samples_per_class_train: number of samples per class used for training\n",
        "        num_samples_per_class_test: number of samples per class used for testing\n",
        "        seed: random seed to ensure consistent results\n",
        "    Returns:\n",
        "        a tuple of (1) images for training (2) labels for training (3) images for testing, and (4) labels for testing\n",
        "            (1) numpy array of shape [num_classes * num_samples_per_class_train, 784], binary pixels\n",
        "            (2) numpy array of shape [num_classes * num_samples_per_class_train], integers of the class label\n",
        "            (3) numpy array of shape [num_classes * num_samples_per_class_test, 784], binary pixels\n",
        "            (4) numpy array of shape [num_classes * num_samples_per_class_test], integers of the class label\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    num_samples_per_class = num_samples_per_class_train + num_samples_per_class_test\n",
        "    assert num_classes <= 1623\n",
        "    assert num_samples_per_class <= 20\n",
        "\n",
        "    # construct folders\n",
        "    data_folder = '/content/drive/MyDrive/Colab Notebooks/Big Data Intelligence/data/omniglot_resized' if is_colab else './omniglot_resized'\n",
        "    character_folders = [os.path.join(data_folder, family, character)\n",
        "                         for family in os.listdir(data_folder)\n",
        "                         if os.path.isdir(os.path.join(data_folder, family))\n",
        "                         for character in os.listdir(os.path.join(data_folder, family))\n",
        "                         if os.path.isdir(os.path.join(data_folder, family, character))]\n",
        "    random.shuffle(character_folders)\n",
        "    if num_classes == -1:\n",
        "        num_classes = len(character_folders)\n",
        "    else:\n",
        "        character_folders = character_folders[:num_classes]\n",
        "\n",
        "    # read images\n",
        "    all_images = np.zeros(shape=(num_samples_per_class, num_classes, 28, 28))\n",
        "    all_labels = np.zeros(shape=(num_samples_per_class, num_classes))\n",
        "    label_images = get_images(character_folders, list(range(num_classes)), nb_samples=num_samples_per_class, shuffle=True)\n",
        "    temp_count = np.zeros(num_classes, dtype=int)\n",
        "    for label, imagefile in label_images:\n",
        "        temp_num = temp_count[label]\n",
        "        all_images[temp_num, label, :, :] = image_file_to_array(imagefile)\n",
        "        all_labels[temp_num, label] = label\n",
        "        temp_count[label] += 1\n",
        "\n",
        "    # split and random permutate\n",
        "    train_image = all_images[:num_samples_per_class_train].reshape(-1, 28, 28)\n",
        "    test_image = all_images[num_samples_per_class_train:].reshape(-1, 28, 28)\n",
        "    train_label = all_labels[:num_samples_per_class_train].reshape(-1)\n",
        "    test_label = all_labels[num_samples_per_class_train:].reshape(-1)\n",
        "    train_image, train_label = pair_shuffle(train_image, train_label)\n",
        "    test_image, test_label = pair_shuffle(test_image, test_label)\n",
        "\n",
        "    if(flatten):\n",
        "        train_image = train_image.reshape(-1, 784)\n",
        "        test_image = test_image.reshape(-1, 784)\n",
        "\n",
        "    return train_image, train_label, test_image, test_label\n",
        "\n",
        "\n",
        "def get_images(paths, labels, nb_samples=None, shuffle=True):\n",
        "    \"\"\"\n",
        "    Takes a set of character folders and labels and returns paths to image files\n",
        "    paired with labels.\n",
        "    Args:\n",
        "        paths: A list of character folders\n",
        "        labels: List or numpy array of same length as paths\n",
        "        nb_samples: Number of images to retrieve per character\n",
        "    Returns:\n",
        "        List of (label, image_path) tuples\n",
        "    \"\"\"\n",
        "    if nb_samples is not None:\n",
        "        sampler = lambda x: random.sample(x, nb_samples)\n",
        "    else:\n",
        "        sampler = lambda x: x\n",
        "    images_labels = [(i, os.path.join(path, image))\n",
        "                     for i, path in zip(labels, paths)\n",
        "                     for image in sampler([pathstr for pathstr in os.listdir(path) if pathstr[-4:] == '.png' ])]\n",
        "    if shuffle:\n",
        "        random.shuffle(images_labels)\n",
        "    return images_labels\n",
        "\n",
        "\n",
        "def image_file_to_array(filename):\n",
        "    \"\"\"\n",
        "    Takes an image path and returns numpy array\n",
        "    Args:\n",
        "        filename: Image filename\n",
        "    Returns:\n",
        "        1 channel image\n",
        "    \"\"\"\n",
        "    # Updated code as function was depricated\n",
        "    image = Image.open(filename).convert('L')\n",
        "    image = np.array(image)\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "    image = 1.0 - image\n",
        "    return image\n",
        "\n",
        "def array_to_image_file(image_array, filename, dim_output):\n",
        "    \"\"\"\n",
        "    Takes a numpy array and saves it as an image file.\n",
        "    Args:\n",
        "        image_array: Flattened numpy array of the image.\n",
        "        filename: File path to save the image (including extension, e.g., .png).\n",
        "        dim_output: Shape of the output image (e.g., [height, width] for 2D images).\n",
        "    \"\"\"\n",
        "    # Reshape the array to the desired dimensions (height x width)\n",
        "    image = image_array.reshape(dim_output)\n",
        "\n",
        "    # Reverse the transformation from image_file_to_array\n",
        "    image = 1.0 - image\n",
        "    image = (image * 255).astype(np.uint8)\n",
        "\n",
        "    # We can now use PIL to save the image\n",
        "    pil_image = Image.fromarray(image)\n",
        "    pil_image.save(filename)\n",
        "\n",
        "def pair_shuffle(array_a, array_b):\n",
        "    \"\"\"\n",
        "    Takes an image array and a label array\n",
        "    Returns:\n",
        "        the shuffled image array and label array\n",
        "    \"\"\"\n",
        "    temp_perm = np.random.permutation(array_a.shape[0])\n",
        "    array_a = array_a[temp_perm]\n",
        "    array_b = array_b[temp_perm]\n",
        "    return array_a, array_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP0ctpODGphp"
      },
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZZjFefcIF6q6"
      },
      "outputs": [],
      "source": [
        "class ConvNN:\n",
        "    def __init__(self, num_classes, num_samples_train, num_samples_test, seed, input_shape=(28, 28, 1), is_colab=False):\n",
        "        self.num_classes = num_classes\n",
        "        self.num_samples_train = num_samples_train\n",
        "        self.num_samples_test = num_samples_test\n",
        "        self.seed = seed\n",
        "        self.input_shape = input_shape\n",
        "        self.model = self.build_model()\n",
        "        self.train_image, self.train_label, self.test_image, self.test_label = LoadData(num_classes, num_samples_train, num_samples_test, seed, False, is_colab)\n",
        "\n",
        "    # The CNN Architecture, inspired by the one given in the homework\n",
        "    def build_model(self):\n",
        "        model = models.Sequential()\n",
        "\n",
        "        # First Conv block: 28x28x1 -> 26x26x32 -> 26x26x32 -> 26x26x32 -> 13x13x32\n",
        "        model.add(layers.Conv2D(32, (3, 3), padding='valid', input_shape=self.input_shape))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.ReLU())\n",
        "        model.add(layers.Conv2D(32, (3, 3), padding='valid'))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.ReLU())\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        # Second Conv block: 13x13x32 -> 11x11x64 -> 11x11x64 -> 11x11x64 -> 5x5x64\n",
        "        model.add(layers.Conv2D(64, (3, 3), padding='valid'))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.ReLU())\n",
        "        model.add(layers.Conv2D(64, (3, 3), padding='valid'))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.ReLU())\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        # Fully connected layers\n",
        "        # Flatten: 5x5x64 -> 1600\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(512))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.ReLU())\n",
        "        model.add(layers.Dropout(0.5))\n",
        "        model.add(layers.Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_and_evaluate(self, model, learning_rate, epochs):\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=learning_rate),\n",
        "            loss=SparseCategoricalCrossentropy(),\n",
        "            metrics=[SparseCategoricalAccuracy()],\n",
        "        )\n",
        "        history = model.fit(\n",
        "            self.train_image,\n",
        "            self.train_label,\n",
        "            epochs=epochs,\n",
        "            validation_data=(self.test_image, self.test_label),\n",
        "            verbose=0,\n",
        "        )\n",
        "        return history\n",
        "\n",
        "    def train_model(self, epochs=500, alpha=1e-5):\n",
        "        history = self.train_and_evaluate(self.model, learning_rate=alpha, epochs=epochs)\n",
        "        return history\n",
        "\n",
        "    def train_multiple_models(self, learning_rates, epochs, print_output=True, predictions=0, plot_graph=True):\n",
        "        results = []\n",
        "        for lr in learning_rates:\n",
        "            for epoch in epochs:\n",
        "                model = self.build_model()\n",
        "                history = self.train_and_evaluate(model, learning_rate=lr, epochs=epoch)\n",
        "                final_val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
        "\n",
        "                if print_output:\n",
        "                    print(f\"{final_val_accuracy:.3f}: {lr:.3f} x {epoch}\")\n",
        "                results.append((lr, epoch, final_val_accuracy))\n",
        "\n",
        "                if predictions > 0:\n",
        "                    for i in range(predictions):\n",
        "                        index = np.random.randint(0, self.test_image.shape[0])\n",
        "                        image = self.test_image[index].reshape(1, 28, 28, 1)\n",
        "                        predicted_class = np.argmax(model.predict(image, verbose=0), axis=1)\n",
        "                        print(f\"Prediction: {predicted_class[0]}, Actual: {self.test_label[index]}\")\n",
        "\n",
        "        if plot_graph:\n",
        "            fig = plt.figure()\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "            learning_rates_plot, epoch_values_plot, val_accuracies_plot = zip(*results)\n",
        "\n",
        "            sc = ax.scatter(learning_rates_plot, epoch_values_plot, val_accuracies_plot, c=val_accuracies_plot, cmap='viridis', s=100)\n",
        "            ax.set_xlabel('Learning Rate')\n",
        "            ax.set_ylabel('Epochs')\n",
        "            ax.set_zlabel('Validation Accuracy')\n",
        "            plt.colorbar(sc, label='Validation Accuracy')\n",
        "            plt.title('Learning Rate vs Epochs vs Validation Accuracy')\n",
        "            plt.show()\n",
        "        return results\n",
        "\n",
        "    def show_n_predictions(self, n=10, print_output=True):\n",
        "        for i in range(n):\n",
        "            index = np.random.randint(0, self.test_image.shape[0])\n",
        "            image = self.test_image[index].reshape(1, 28, 28, 1)\n",
        "            predicted_class = np.argmax(self.model.predict(image, verbose=0), axis=1)\n",
        "            print(f\"Predicted class: {predicted_class}, Actual class: {self.test_label[index]}\")\n",
        "\n",
        "        if(print_output):\n",
        "            test_loss, test_acc = self.model.evaluate(self.test_image, self.test_label, verbose=2)\n",
        "            print(f\"Test accuracy: {test_acc}\")\n",
        "            print(f\"Test loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tEcr39VGxwL"
      },
      "source": [
        "# Fully Connected Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4Nl3O82JG0Rb"
      },
      "outputs": [],
      "source": [
        "class ConnectedNN_1_layers:\n",
        "    def __init__(self, num_classes, num_samples_train, num_samples_test, seed, is_colab=False):\n",
        "        self.num_classes = num_classes\n",
        "        self.num_samples_train = num_samples_train\n",
        "        self.num_samples_test = num_samples_test\n",
        "        self.seed = seed\n",
        "        self.model = self.build_model()\n",
        "        self.train_image, self.train_label, self.test_image, self.test_label = LoadData(num_classes, num_samples_train, num_samples_test, seed, True, is_colab)\n",
        "\n",
        "    # The FCNN Architecture\n",
        "    def build_model(self):\n",
        "        model = models.Sequential()\n",
        "\n",
        "        # Update input shape to match flattened input\n",
        "        model.add(layers.Input(shape=(784,)))  # Flattened shape of 28x28\n",
        "\n",
        "        # Add Dense layers with ReLU activation\n",
        "        model.add(layers.Dense(512, activation='relu'))\n",
        "        model.add(layers.Dropout(0.3))\n",
        "\n",
        "        # Output layer with softmax for classification\n",
        "        model.add(layers.Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_and_evaluate(self, model, learning_rate, epochs):\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=learning_rate),\n",
        "            loss=SparseCategoricalCrossentropy(),\n",
        "            metrics=[SparseCategoricalAccuracy()],\n",
        "        )\n",
        "        history = model.fit(\n",
        "            self.train_image,\n",
        "            self.train_label,\n",
        "            epochs=epochs,\n",
        "            validation_data=(self.test_image, self.test_label),\n",
        "            verbose=0,\n",
        "        )\n",
        "        return history\n",
        "\n",
        "    def train_model(self, epochs=500, alpha=1e-5):\n",
        "        history = self.train_and_evaluate(self.model, learning_rate=alpha, epochs=epochs)\n",
        "        return history\n",
        "\n",
        "    def train_multiple_models(self, learning_rates, epochs, print_output=True, predictions=0, plot_graph=True):\n",
        "        results = []\n",
        "        for lr in learning_rates:\n",
        "            for epoch in epochs:\n",
        "                model = self.build_model()\n",
        "                history = self.train_and_evaluate(model, learning_rate=lr, epochs=epoch)\n",
        "                final_val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
        "\n",
        "                if print_output:\n",
        "                    print(f\"{final_val_accuracy:.3f}: {lr:.3f} x {epoch}\")\n",
        "                results.append((lr, epoch, final_val_accuracy))\n",
        "\n",
        "                if predictions > 0:\n",
        "                  for i in range(predictions):\n",
        "                      index = np.random.randint(0, self.test_image.shape[0])\n",
        "                      image = self.test_image[index].reshape(1, 784)\n",
        "                      predicted_class = np.argmax(model.predict(image, verbose=0), axis=1)\n",
        "                      print(f\"Prediction: {predicted_class[0]}, Actual: {self.test_label[index]}\")\n",
        "\n",
        "\n",
        "\n",
        "        if plot_graph:\n",
        "            fig = plt.figure()\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "            learning_rates_plot, epoch_values_plot, val_accuracies_plot = zip(*results)\n",
        "\n",
        "            sc = ax.scatter(learning_rates_plot, epoch_values_plot, val_accuracies_plot, c=val_accuracies_plot, cmap='viridis', s=100)\n",
        "            ax.set_xlabel('Learning Rate')\n",
        "            ax.set_ylabel('Epochs')\n",
        "            ax.set_zlabel('Validation Accuracy')\n",
        "            plt.colorbar(sc, label='Validation Accuracy')\n",
        "            plt.title('Learning Rate vs Epochs vs Validation Accuracy')\n",
        "            plt.show()\n",
        "        return results\n",
        "\n",
        "    def show_n_predictions(self, n=10, print_output=True):\n",
        "        for i in range(n):\n",
        "            index = np.random.randint(0, self.test_image.shape[0])\n",
        "            image = self.test_image[index].reshape(1, 784)\n",
        "            prediction = self.model.predict(image, verbose=0)\n",
        "            print(f\"Predicted class: {np.argmax(prediction)}, Actual class: {self.test_label[index]}\")\n",
        "\n",
        "        if(print_output):\n",
        "            test_loss, test_acc = self.model.evaluate(self.test_image, self.test_label, verbose=2)\n",
        "            print(f\"Test accuracy: {test_acc}\")\n",
        "            print(f\"Test loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "duKYZRLI9Wvg"
      },
      "outputs": [],
      "source": [
        "class ConnectedNN_2_layers:\n",
        "    def __init__(self, num_classes, num_samples_train, num_samples_test, seed, is_colab=False):\n",
        "        self.num_classes = num_classes\n",
        "        self.num_samples_train = num_samples_train\n",
        "        self.num_samples_test = num_samples_test\n",
        "        self.seed = seed\n",
        "        self.model = self.build_model()\n",
        "        self.train_image, self.train_label, self.test_image, self.test_label = LoadData(num_classes, num_samples_train, num_samples_test, seed, True, is_colab)\n",
        "\n",
        "    # The FCNN Architecture\n",
        "    def build_model(self):\n",
        "        model = models.Sequential()\n",
        "\n",
        "        # Update input shape to match flattened input\n",
        "        model.add(layers.Input(shape=(784,)))  # Flattened shape of 28x28\n",
        "\n",
        "        # Add Dense layers with ReLU activation\n",
        "        model.add(layers.Dense(512, activation='relu'))\n",
        "        model.add(layers.Dropout(0.3))\n",
        "        model.add(layers.Dense(256, activation='relu'))\n",
        "        model.add(layers.Dropout(0.3))\n",
        "\n",
        "        # Output layer with softmax for classification\n",
        "        model.add(layers.Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_and_evaluate(self, model, learning_rate, epochs):\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=learning_rate),\n",
        "            loss=SparseCategoricalCrossentropy(),\n",
        "            metrics=[SparseCategoricalAccuracy()],\n",
        "        )\n",
        "        history = model.fit(\n",
        "            self.train_image,\n",
        "            self.train_label,\n",
        "            epochs=epochs,\n",
        "            validation_data=(self.test_image, self.test_label),\n",
        "            verbose=0,\n",
        "        )\n",
        "        return history\n",
        "\n",
        "    def train_model(self, epochs=500, alpha=1e-5):\n",
        "        history = self.train_and_evaluate(self.model, learning_rate=alpha, epochs=epochs)\n",
        "        return history\n",
        "\n",
        "    def train_multiple_models(self, learning_rates, epochs, print_output=True, predictions=0, plot_graph=True):\n",
        "        results = []\n",
        "        for lr in learning_rates:\n",
        "            for epoch in epochs:\n",
        "                model = self.build_model()\n",
        "                history = self.train_and_evaluate(model, learning_rate=lr, epochs=epoch)\n",
        "                final_val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
        "\n",
        "                if print_output:\n",
        "                    print(f\"{final_val_accuracy:.3f}: {lr:.3f} x {epoch}\")\n",
        "                results.append((lr, epoch, final_val_accuracy))\n",
        "\n",
        "                if predictions > 0:\n",
        "                  for i in range(predictions):\n",
        "                      index = np.random.randint(0, self.test_image.shape[0])\n",
        "                      image = self.test_image[index].reshape(1, 784)\n",
        "                      predicted_class = np.argmax(model.predict(image, verbose=0), axis=1)\n",
        "                      print(f\"Prediction: {predicted_class[0]}, Actual: {self.test_label[index]}\")\n",
        "\n",
        "\n",
        "\n",
        "        if plot_graph:\n",
        "            fig = plt.figure()\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "            learning_rates_plot, epoch_values_plot, val_accuracies_plot = zip(*results)\n",
        "\n",
        "            sc = ax.scatter(learning_rates_plot, epoch_values_plot, val_accuracies_plot, c=val_accuracies_plot, cmap='viridis', s=100)\n",
        "            ax.set_xlabel('Learning Rate')\n",
        "            ax.set_ylabel('Epochs')\n",
        "            ax.set_zlabel('Validation Accuracy')\n",
        "            plt.colorbar(sc, label='Validation Accuracy')\n",
        "            plt.title('Learning Rate vs Epochs vs Validation Accuracy')\n",
        "            plt.show()\n",
        "        return results\n",
        "\n",
        "    def show_n_predictions(self, n=10, print_output=True):\n",
        "        for i in range(n):\n",
        "            index = np.random.randint(0, self.test_image.shape[0])\n",
        "            image = self.test_image[index].reshape(1, 784)\n",
        "            prediction = self.model.predict(image, verbose=0)\n",
        "            print(f\"Predicted class: {np.argmax(prediction)}, Actual class: {self.test_label[index]}\")\n",
        "\n",
        "        if(print_output):\n",
        "            test_loss, test_acc = self.model.evaluate(self.test_image, self.test_label, verbose=2)\n",
        "            print(f\"Test accuracy: {test_acc}\")\n",
        "            print(f\"Test loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RLaecUFEGpPD"
      },
      "outputs": [],
      "source": [
        "class ConnectedNN_3_layers:\n",
        "    def __init__(self, num_classes, num_samples_train, num_samples_test, seed, is_colab=False):\n",
        "        self.num_classes = num_classes\n",
        "        self.num_samples_train = num_samples_train\n",
        "        self.num_samples_test = num_samples_test\n",
        "        self.seed = seed\n",
        "        self.model = self.build_model()\n",
        "        self.train_image, self.train_label, self.test_image, self.test_label = LoadData(num_classes, num_samples_train, num_samples_test, seed, True, is_colab)\n",
        "\n",
        "    # The FCNN Architecture\n",
        "    def build_model(self):\n",
        "        model = models.Sequential()\n",
        "\n",
        "        # Update input shape to match flattened input\n",
        "        model.add(layers.Input(shape=(784,)))  # Flattened shape of 28x28\n",
        "\n",
        "        # Add Dense layers with ReLU activation\n",
        "        model.add(layers.Dense(512, activation='relu'))\n",
        "        model.add(layers.Dropout(0.3))\n",
        "        model.add(layers.Dense(256, activation='relu'))\n",
        "        model.add(layers.Dropout(0.3))\n",
        "        model.add(layers.Dense(128, activation='relu'))\n",
        "        #model.add(layers.Dropout(0.3))\n",
        "        #model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "        # Output layer with softmax for classification\n",
        "        model.add(layers.Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_and_evaluate(self, model, learning_rate, epochs):\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=learning_rate),\n",
        "            loss=SparseCategoricalCrossentropy(),\n",
        "            metrics=[SparseCategoricalAccuracy()],\n",
        "        )\n",
        "        history = model.fit(\n",
        "            self.train_image,\n",
        "            self.train_label,\n",
        "            epochs=epochs,\n",
        "            validation_data=(self.test_image, self.test_label),\n",
        "            verbose=0,\n",
        "        )\n",
        "        return history\n",
        "\n",
        "    def train_model(self, epochs=500, alpha=1e-5):\n",
        "        history = self.train_and_evaluate(self.model, learning_rate=alpha, epochs=epochs)\n",
        "        return history\n",
        "\n",
        "    def train_multiple_models(self, learning_rates, epochs, print_output=True, predictions=0, plot_graph=True):\n",
        "        results = []\n",
        "        for lr in learning_rates:\n",
        "            for epoch in epochs:\n",
        "                model = self.build_model()\n",
        "                history = self.train_and_evaluate(model, learning_rate=lr, epochs=epoch)\n",
        "                final_val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
        "\n",
        "                if print_output:\n",
        "                    print(f\"{final_val_accuracy:.3f}: {lr:.3f} x {epoch}\")\n",
        "                results.append((lr, epoch, final_val_accuracy))\n",
        "\n",
        "                if predictions > 0:\n",
        "                  for i in range(predictions):\n",
        "                      index = np.random.randint(0, self.test_image.shape[0])\n",
        "                      image = self.test_image[index].reshape(1, 784)\n",
        "                      predicted_class = np.argmax(model.predict(image, verbose=0), axis=1)\n",
        "                      print(f\"Prediction: {predicted_class[0]}, Actual: {self.test_label[index]}\")\n",
        "\n",
        "\n",
        "\n",
        "        if plot_graph:\n",
        "            fig = plt.figure()\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "            learning_rates_plot, epoch_values_plot, val_accuracies_plot = zip(*results)\n",
        "\n",
        "            sc = ax.scatter(learning_rates_plot, epoch_values_plot, val_accuracies_plot, c=val_accuracies_plot, cmap='viridis', s=100)\n",
        "            ax.set_xlabel('Learning Rate')\n",
        "            ax.set_ylabel('Epochs')\n",
        "            ax.set_zlabel('Validation Accuracy')\n",
        "            plt.colorbar(sc, label='Validation Accuracy')\n",
        "            plt.title('Learning Rate vs Epochs vs Validation Accuracy')\n",
        "            plt.show()\n",
        "        return results\n",
        "\n",
        "    def show_n_predictions(self, n=10, print_output=True):\n",
        "        for i in range(n):\n",
        "            index = np.random.randint(0, self.test_image.shape[0])\n",
        "            image = self.test_image[index].reshape(1, 784)\n",
        "            prediction = self.model.predict(image, verbose=0)\n",
        "            print(f\"Predicted class: {np.argmax(prediction)}, Actual class: {self.test_label[index]}\")\n",
        "\n",
        "        if(print_output):\n",
        "            test_loss, test_acc = self.model.evaluate(self.test_image, self.test_label, verbose=2)\n",
        "            print(f\"Test accuracy: {test_acc}\")\n",
        "            print(f\"Test loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2jvw4h6G3kI"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "2z2YGC4zG5UR",
        "outputId": "0db62f6b-9094-4304-c065-fcc590d6d508"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 3] Systemet finner ikke angitt bane: '/content/drive/MyDrive/Colab Notebooks/Big Data Intelligence/data/omniglot_resized'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Build and summarize the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_conv \u001b[38;5;241m=\u001b[39m \u001b[43mConvNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_colab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define ranges for ConvNN\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 0.024: 0.500 x 750\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 0.592: 0.100 x 250\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 0.856: 0.050 x 750\u001b[39;00m\n\u001b[0;32m      8\u001b[0m learning_rates_conv \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1e-1\u001b[39m, \u001b[38;5;241m1e-2\u001b[39m, \u001b[38;5;241m1e-3\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m]\n",
            "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36mConvNN.__init__\u001b[1;34m(self, num_classes, num_samples_train, num_samples_test, seed, input_shape, is_colab)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_model()\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_image, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_label, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_image, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_label \u001b[38;5;241m=\u001b[39m \u001b[43mLoadData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_colab\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[2], line 25\u001b[0m, in \u001b[0;36mLoadData\u001b[1;34m(num_classes, num_samples_per_class_train, num_samples_per_class_test, seed, flatten, is_colab)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# construct folders\u001b[39;00m\n\u001b[0;32m     23\u001b[0m data_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/Colab Notebooks/Big Data Intelligence/data/omniglot_resized\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_colab \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./omniglot_resized\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     24\u001b[0m character_folders \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_folder, family, character)\n\u001b[1;32m---> 25\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m family \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m                      \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_folder, family))\n\u001b[0;32m     27\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m character \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_folder, family))\n\u001b[0;32m     28\u001b[0m                      \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_folder, family, character))]\n\u001b[0;32m     29\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(character_folders)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_classes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Systemet finner ikke angitt bane: '/content/drive/MyDrive/Colab Notebooks/Big Data Intelligence/data/omniglot_resized'"
          ]
        }
      ],
      "source": [
        "# Build and summarize the model\n",
        "model_conv = ConvNN(num_classes=50, num_samples_train=15, num_samples_test=5, seed=1, is_colab=True)\n",
        "\n",
        "# Define ranges for ConvNN\n",
        "# 0.024: 0.500 x 750\n",
        "# 0.592: 0.100 x 250\n",
        "# 0.856: 0.050 x 750\n",
        "learning_rates_conv = [1.0, 1e-1, 1e-2, 1e-3, 1e-4]\n",
        "epoch_values_conv = [500, 1000, 2000]\n",
        "\n",
        "\"\"\"\n",
        "[(0.05, 500, 0.8960000276565552),\n",
        " (0.05, 1000, 0.9120000004768372),\n",
        " (0.05, 1500, 0.9079999923706055),\n",
        " (0.01, 500, 0.8999999761581421),\n",
        " (0.01, 1000, 0.8999999761581421),\n",
        " (0.01, 1500, 0.9240000247955322), <- Best\n",
        " (0.005, 500, 0.8759999871253967),\n",
        " (0.005, 1000, 0.8840000033378601),\n",
        " (0.005, 1500, 0.9120000004768372),\n",
        " (0.001, 500, 0.8840000033378601),\n",
        " (0.001, 1000, 0.8799999952316284),\n",
        " (0.001, 1500, 0.8960000276565552)]\n",
        "\"\"\"\n",
        "\n",
        "# Training the model\n",
        "model_conv.train_multiple_models(learning_rates_conv, epoch_values_conv, print_output=True, predictions=5, plot_graph=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgeNFs9BEh-0"
      },
      "outputs": [],
      "source": [
        "# Build and summarize the model\n",
        "model_conn_1 = ConnectedNN_1_layers(num_classes=50, num_samples_train=15, num_samples_test=5, seed=1, is_colab=True)\n",
        "model_conn_2 = ConnectedNN_2_layers(num_classes=50, num_samples_train=15, num_samples_test=5, seed=1, is_colab=True)\n",
        "model_conn_3 = ConnectedNN_3_layers(num_classes=50, num_samples_train=15, num_samples_test=5, seed=1, is_colab=True)\n",
        "\n",
        "# Define ranges for ConnectedNN\n",
        "learning_rates_conn = [1.0, 1e-1, 1e-2, 1e-3, 1e-4]\n",
        "epoch_values_conn = [500, 1000, 2000]\n",
        "\n",
        "# Train the model\n",
        "result1 = model_conn_1.train_multiple_models(learning_rates_conn, epoch_values_conn, print_output=True, predictions=5, plot_graph=False)\n",
        "result2 = model_conn_2.train_multiple_models(learning_rates_conn, epoch_values_conn, print_output=True, predictions=5, plot_graph=False)\n",
        "result3 = model_conn_3.train_multiple_models(learning_rates_conn, epoch_values_conn, print_output=True, predictions=5, plot_graph=False)\n",
        "\n",
        "\"\"\"\n",
        "With 1 Dense layer\n",
        "[(1.0, 500, 0.11599999666213989),\n",
        " (1.0, 1000, 0.04399999976158142),\n",
        " (1.0, 2000, 0.07599999755620956),\n",
        " (0.1, 500, 0.1080000028014183),\n",
        " (0.1, 1000, 0.1120000034570694),\n",
        " (0.1, 2000, 0.10000000149011612),\n",
        " (0.01, 500, 0.4880000054836273),\n",
        " (0.01, 1000, 0.5320000052452087),\n",
        " (0.01, 2000, 0.5479999780654907),\n",
        " (0.001, 500, 0.4880000054836273),\n",
        " (0.001, 1000, 0.47600001096725464),\n",
        " (0.001, 2000, 0.5239999890327454),\n",
        " (0.0001, 500, 0.5199999809265137),\n",
        " (0.0001, 1000, 0.5),\n",
        " (0.0001, 2000, 0.5199999809265137)]\n",
        "\n",
        "With 2 Dense layers\n",
        "[(1.0, 500, 0.04399999976158142),\n",
        " (1.0, 1000, 0.052000001072883606),\n",
        " (1.0, 2000, 0.03200000151991844),\n",
        " (0.1, 500, 0.035999998450279236),\n",
        " (0.1, 1000, 0.05999999865889549),\n",
        " (0.1, 2000, 0.019999999552965164),\n",
        " (0.01, 500, 0.5239999890327454),\n",
        " (0.01, 1000, 0.5239999890327454),\n",
        " (0.01, 2000, 0.5320000052452087),\n",
        " (0.001, 500, 0.5120000243186951),\n",
        " (0.001, 1000, 0.4959999918937683),\n",
        " (0.001, 2000, 0.5040000081062317),\n",
        " (0.0001, 500, 0.4959999918937683),\n",
        " (0.0001, 1000, 0.527999997138977),\n",
        " (0.0001, 2000, 0.4880000054836273)]\n",
        "\n",
        "With 3 Dense layers\n",
        "[(1.0, 500, 0.019999999552965164),\n",
        " (1.0, 1000, 0.019999999552965164),\n",
        " (1.0, 2000, 0.019999999552965164),\n",
        " (0.1, 500, 0.019999999552965164),\n",
        " (0.1, 1000, 0.019999999552965164),\n",
        " (0.1, 2000, 0.019999999552965164),\n",
        " (0.01, 500, 0.13600000739097595),\n",
        " (0.01, 1000, 0.15199999511241913),\n",
        " (0.01, 2000, 0.024000000208616257),\n",
        " (0.001, 500, 0.4480000138282776),\n",
        " (0.001, 1000, 0.5040000081062317),\n",
        " (0.001, 2000, 0.515999972820282),\n",
        " (0.0001, 500, 0.492000013589859),\n",
        " (0.0001, 1000, 0.47200000286102295),\n",
        " (0.0001, 2000, 0.47200000286102295)]\n",
        "\n",
        "\"\"\"\n",
        "print(\"With 1 Dense Layer\")\n",
        "print(result1)\n",
        "print(\"With 2 Dense Layers\")\n",
        "print(result2)\n",
        "print(\"With 3 Dense Layers\")\n",
        "print(result3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY6WBa1zKnhA"
      },
      "source": [
        "# Experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikQYLoxCOxIj"
      },
      "source": [
        "## Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "e7HYYtWBKpD1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/80 Split\n",
            "0.4480000138282776\n",
            "50/50 Split\n",
            "0.36399999260902405\n",
            "80/20 Split\n",
            "0.4300000071525574\n",
            "90/10 Split\n",
            "0.4099999964237213\n"
          ]
        }
      ],
      "source": [
        "model_conn_20_80 = ConnectedNN_1_layers(num_classes=50, num_samples_train=15, num_samples_test=5, seed=1, is_colab=False)\n",
        "model_conn_50_50 = ConnectedNN_1_layers(num_classes=50, num_samples_train=10, num_samples_test=10, seed=1, is_colab=False)\n",
        "model_conn_80_20 = ConnectedNN_1_layers(num_classes=50, num_samples_train=16, num_samples_test=4, seed=1, is_colab=False)\n",
        "model_conn_90_10 = ConnectedNN_1_layers(num_classes=50, num_samples_train=18, num_samples_test=2, seed=1, is_colab=False)\n",
        "\n",
        "best_lr     = 0.01\n",
        "best_epoch  = 2000\n",
        "\n",
        "result_20_80 = model_conn_20_80.train_model(best_epoch, best_lr)\n",
        "result_50_50 = model_conn_50_50.train_model(best_epoch, best_lr)\n",
        "result_80_20 = model_conn_80_20.train_model(best_epoch, best_lr)\n",
        "result_90_10 = model_conn_90_10.train_model(best_epoch, best_lr)\n",
        "\n",
        "names = [\"20/80 Split\", \"50/50 Split\", \"80/20 Split\", \"90/10 Split\"]\n",
        "\n",
        "\"\"\"\n",
        "20/80 Split\n",
        "0.5640000104904175\n",
        "50/50 Split\n",
        "0.4320000112056732\n",
        "80/20 Split\n",
        "0.5199999809265137\n",
        "90/10 Split\n",
        "0.5600000023841858\n",
        "\"\"\"\n",
        "\n",
        "for i, item in enumerate([result_20_80, result_50_50, result_80_20, result_90_10]):\n",
        "  print(names[i])\n",
        "  print(item.history['val_sparse_categorical_accuracy'][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aReS6JsvO0L6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/80 Split\n",
            "0.8799999952316284\n",
            "50/50 Split\n",
            "0.8059999942779541\n",
            "80/20 Split\n",
            "0.8399999737739563\n",
            "90/10 Split\n",
            "0.8700000047683716\n"
          ]
        }
      ],
      "source": [
        "model_conv_20_80 = ConvNN(num_classes=50, num_samples_train=15, num_samples_test=5, seed=1, is_colab=False)\n",
        "model_conv_50_50 = ConvNN(num_classes=50, num_samples_train=10, num_samples_test=10, seed=1, is_colab=False)\n",
        "model_conv_80_20 = ConvNN(num_classes=50, num_samples_train=16, num_samples_test=4, seed=1, is_colab=False)\n",
        "model_conv_90_10 = ConvNN(num_classes=50, num_samples_train=18, num_samples_test=2, seed=1, is_colab=False)\n",
        "\n",
        "best_lr     = 0.01\n",
        "best_epoch  = 2000\n",
        "\n",
        "result_20_80 = model_conv_20_80.train_model(best_epoch, best_lr)\n",
        "result_50_50 = model_conv_50_50.train_model(best_epoch, best_lr)\n",
        "result_80_20 = model_conv_80_20.train_model(best_epoch, best_lr)\n",
        "result_90_10 = model_conv_90_10.train_model(best_epoch, best_lr)\n",
        "\n",
        "names = [\"20/80 Split\", \"50/50 Split\", \"80/20 Split\", \"90/10 Split\"]\n",
        "\n",
        "\"\"\"\n",
        "20/80 Split\n",
        "0.8799999952316284\n",
        "50/50 Split\n",
        "0.8059999942779541\n",
        "80/20 Split\n",
        "0.8399999737739563\n",
        "90/10 Split\n",
        "0.8700000047683716\n",
        "\"\"\"\n",
        "\n",
        "for i, item in enumerate([result_20_80, result_50_50, result_80_20, result_90_10]):\n",
        "  print(names[i])\n",
        "  print(item.history['val_sparse_categorical_accuracy'][-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Y-fAQRO0nt"
      },
      "source": [
        "## Data Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IhYOe5yxOu3h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 Categories\n",
            "0.6399999856948853\n",
            "25 Categories\n",
            "0.4880000054836273\n",
            "75 Categories\n",
            "0.4320000112056732\n"
          ]
        }
      ],
      "source": [
        "model_conn_10 = ConnectedNN_1_layers(num_classes=10, num_samples_train=15, num_samples_test=5, seed=1, is_colab=False)\n",
        "model_conn_25 = ConnectedNN_1_layers(num_classes=25, num_samples_train=15, num_samples_test=5, seed=1, is_colab=False)\n",
        "model_conn_75 = ConnectedNN_1_layers(num_classes=75, num_samples_train=15, num_samples_test=5, seed=1, is_colab=False)\n",
        "\n",
        "best_lr     = 0.01\n",
        "best_epoch  = 2000\n",
        "\n",
        "result_10 = model_conn_10.train_model(best_epoch, best_lr)\n",
        "result_25 = model_conn_25.train_model(best_epoch, best_lr)\n",
        "result_75 = model_conn_75.train_model(best_epoch, best_lr)\n",
        "\n",
        "names = [\"10 Categories\", \"25 Categories\", \"75 Categories\"]\n",
        "\n",
        "\"\"\"\n",
        "10 Categories\n",
        "0.6399999856948853\n",
        "25 Categories\n",
        "0.4880000054836273\n",
        "75 Categories\n",
        "0.4320000112056732\n",
        "\"\"\"\n",
        "\n",
        "for i, item in enumerate([result_10, result_25, result_75]):\n",
        "  print(names[i])\n",
        "  print(item.history['val_sparse_categorical_accuracy'][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HalKq2rrP7nm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 Categories\n",
            "0.8999999761581421\n",
            "25 Categories\n",
            "0.8880000114440918\n",
            "75 Categories\n",
            "0.8799999952316284\n"
          ]
        }
      ],
      "source": [
        "model_conv_10 = ConvNN(num_classes=10, num_samples_train=15, num_samples_test=5, seed=1, is_colab=False)\n",
        "model_conv_25 = ConvNN(num_classes=25, num_samples_train=15, num_samples_test=5, seed=1, is_colab=False)\n",
        "model_conv_75 = ConvNN(num_classes=75, num_samples_train=15, num_samples_test=5, seed=1, is_colab=False)\n",
        "\n",
        "best_lr     = 0.01\n",
        "best_epoch  = 2000\n",
        "\n",
        "result_10 = model_conv_10.train_model(best_epoch, best_lr)\n",
        "result_25 = model_conv_25.train_model(best_epoch, best_lr)\n",
        "result_75 = model_conv_75.train_model(best_epoch, best_lr)\n",
        "\n",
        "names = [\"10 Categories\", \"25 Categories\", \"75 Categories\"]\n",
        "\n",
        "\"\"\"\n",
        "10 Categories\n",
        "0.8999999761581421\n",
        "25 Categories\n",
        "0.8880000114440918\n",
        "75 Categories\n",
        "0.8799999952316284\n",
        "\"\"\"\n",
        "\n",
        "for i, item in enumerate([result_10, result_25, result_75]):\n",
        "  print(names[i])\n",
        "  print(item.history['val_sparse_categorical_accuracy'][-1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "F7GY2Z24H3eq",
        "TP0ctpODGphp",
        "3tEcr39VGxwL"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
